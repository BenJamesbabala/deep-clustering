# -*- coding: utf-8 -*-
"""
Created on Mon Sep 26 15:23:31 2016

@author: jcsilva
"""
import numpy as np
import random
import soundfile as sf
from python_speech_features import sigproc
from config import FRAME_LENGTH, FRAME_SHIFT, FRAME_RATE
from config import TIMESTEPS, DB_THRESHOLD, EMBEDDINGS_DIMENSION


def squared_hann(M):
    return np.sqrt(np.hanning(M))


def stft(sig, rate):
    frames = sigproc.framesig(sig,
                              FRAME_LENGTH*rate,
                              FRAME_SHIFT*rate,
                              winfunc=squared_hann)
    spec = np.fft.rfft(frames, int(FRAME_LENGTH*rate))
    # adding 1e-7 just to avoid problems with log(0)
    return np.log10(np.absolute(spec)+1e-7)  # Log 10 for easier dB calculation


def matrix_euclidean(X, y):
    """
    Computes the euclidean distances of the last axis of a matrix and a vector.
    """
    d = X - y
    d **= 2
    d = np.sum(d, axis=-1)
    d = np.sqrt(d)
    return d


def prepare_enhancement_input(V, spec, centroids, target):
    """
    Prepare input for the enhancement network

    V: embeddings generated by the 1st stage nnet
    spec: the log spectrum for the input
    centroids: list of centroids from kmeans clustering
    target: index of the target represented by one of the given centroids
    """
    X = spec
    V = V.reshape(X.shape + (-1,))
    cdists = []
    for c in centroids:
        cdists.append(matrix_euclidean(V, c))
    cdists = np.argmin(np.array(cdists), axis=0)
    sep = np.copy(X)
    sep[cdists != target] = -7
    E = np.concatenate([X, sep], axis=-1)
    return E


def get_egs(wavlist, min_mix=2, max_mix=3, batch_size=1):
    """
    Generate examples for the neural network from a list of wave files with
    speaker ids. Each line is of type "path speaker", as follows:

    path/to/1st.wav spk1
    path/to/2nd.wav spk2
    path/to/3rd.wav spk1

    and so on.
    min_mix and max_mix are the minimum and maximum number of examples to
    be mixed for generating a training example
    """
    speaker_wavs = {}
    batch_x = []    # Input
    batch_y = []    # Output for the KMeans objective
    batch_count = 0

    while True:  # Generate examples indefinitely
        # Select number of files to mix
        k = np.random.randint(min_mix, max_mix+1)
        if k > len(speaker_wavs):
            # Reading wav files list and separating per speaker
            speaker_wavs = {}
            f = open(wavlist)
            for line in f:
                line = line.strip().split()
                if len(line) != 2:
                    continue
                p, spk = line
                if spk not in speaker_wavs:
                    speaker_wavs[spk] = []
                speaker_wavs[spk].append(p)
            f.close()
            # Randomizing wav lists
            for spk in speaker_wavs:
                random.shuffle(speaker_wavs[spk])
        wavsum = None
        sigs = []

        # Pop wav files from random speakers, store them individually for
        # dominant spectra decision and generate the mixed input
        for spk in random.sample(speaker_wavs.keys(), k):
            p = speaker_wavs[spk].pop()
            if not speaker_wavs[spk]:
                del(speaker_wavs[spk])  # Remove empty speakers from dictionary
            sig, rate = sf.read(p)
            if rate != FRAME_RATE:
                raise Exception("Config specifies " + str(FRAME_RATE) +
                                "Hz as sample rate, but file " + str(p) +
                                "is in " + str(rate) + "Hz.")
            sig = sig - np.mean(sig)
            sig = sig/np.max(np.abs(sig))
            sig *= (np.random.random()*1/4 + 3/4)
            if wavsum is None:
                wavsum = sig
            else:
                wavsum = wavsum[:len(sig)] + sig[:len(wavsum)]
            sigs.append(sig)

        # STFT for mixed signal
        X = stft(wavsum, rate)
        if len(X) <= TIMESTEPS:
            continue

        # STFTs for individual signals
        specs = []
        for sig in sigs:
            specs.append(stft(sig[:len(wavsum)], rate))
        specs = np.array(specs)

        nc = max_mix

        # Get dominant spectra indexes, create one-hot outputs
#        Y = np.zeros(X.shape + (nc,))
#        vals = np.argmax(specs, axis=0)
#        for i in range(k):
#            t = np.zeros(nc)
#            t[i] = 1
#            Y[vals == i] = t
#
#        # Create mask for zeroing out gradients from silence components
#        m = np.max(X) - DB_THRESHOLD/20.  # Minus 40dB
#        z = np.zeros(nc)
#        Y[X < m] = z
        Y = np.zeros(X.shape + (nc,))
        Y[:, :, :len(specs)] = np.transpose(specs, (1, 2, 0))
        norm = np.linalg.norm(Y, axis=2, keepdims=True)
        Y = Y/norm

        # Generating sequences
        i = 0
        while i + TIMESTEPS < len(X):
            batch_x.append(X[i:i+TIMESTEPS])
            batch_y.append(Y[i:i+TIMESTEPS])
            i += TIMESTEPS//2

            batch_count += 1

            if batch_count == batch_size:
                inp = np.array(batch_x).reshape((batch_size,
                                                 TIMESTEPS, -1))
                out = np.array(batch_y).reshape((batch_size,
                                                 TIMESTEPS, -1))
                yield({'input': inp},
                      {'kmeans_o': out})
                batch_x = []
                batch_y = []
                batch_count = 0


def get_egs_end2end(wavlist, model, min_mix=2, max_mix=3, batch_size=1):
    """
    Generates egs for the end-to-end
    """
    speaker_wavs = {}
    batch_x = []    # Input
    batch_s = []    # Output
    batch_count = 0

    while True:  # Generate examples indefinitely
        # Select number of files to mix
        k = np.random.randint(min_mix, max_mix+1)
        if k > len(speaker_wavs):
            # Reading wav files list and separating per speaker
            speaker_wavs = {}
            f = open(wavlist)
            for line in f:
                line = line.strip().split()
                if len(line) != 2:
                    continue
                p, spk = line
                if spk not in speaker_wavs:
                    speaker_wavs[spk] = []
                speaker_wavs[spk].append(p)
            f.close()
            # Randomizing wav lists
            for spk in speaker_wavs:
                random.shuffle(speaker_wavs[spk])
        wavsum = None
        sigs = []

        # Pop wav files from random speakers, store them individually for
        # dominant spectra decision and generate the mixed input
        for spk in random.sample(speaker_wavs.keys(), k):
            p = speaker_wavs[spk].pop()
            if not speaker_wavs[spk]:
                del(speaker_wavs[spk])  # Remove empty speakers from dictionary
            sig, rate = sf.read(p)
            if rate != FRAME_RATE:
                raise Exception("Config specifies " + str(FRAME_RATE) +
                                "Hz as sample rate, but file " + str(p) +
                                "is in " + str(rate) + "Hz.")
            sig = sig - np.mean(sig)
            sig = sig/np.max(np.abs(sig))
            sig *= (np.random.random()*1/4 + 3/4)
            if wavsum is None:
                wavsum = sig
            else:
                wavsum = wavsum[:len(sig)] + sig[:len(wavsum)]
            sigs.append(sig)

        # STFT for mixed signal
        X = stft(wavsum, rate)
        if len(X) <= TIMESTEPS:
            continue

        # STFTs for individual signals
        specs = []
        for sig in sigs:
            specs.append(stft(sig[:len(wavsum)], rate))
        specs = np.array(specs)

        # Get a random source's centroid
        V = model.predict(X.reshape((1, -1, int(model.input.get_shape()[-1]))))
        V = V.reshape((-1, EMBEDDINGS_DIMENSION))
        vals = np.argmax(specs, axis=0)
        vals = vals.reshape(V.shape[:-1])
        r = random.randint(0, k-1)
        cs = []
        for i in range(k):
            cs.append(np.sum(V[vals == i], axis=0))
        S = specs[r]  # Output to be reconstructed
        E = prepare_enhancement_input(V, X, cs, r)

        # Generating sequences
        i = 0
        while i + TIMESTEPS < len(X):
            batch_x.append(E[i:i+TIMESTEPS])
            batch_s.append(S[i:i+TIMESTEPS])
            i += TIMESTEPS//2

            batch_count += 1

            if batch_count == batch_size:
                inp = np.array(batch_x).reshape((batch_size,
                                                 TIMESTEPS, -1))
                rec = np.array(batch_s).reshape((batch_size,
                                                 TIMESTEPS, -1))
                yield({'input': inp},
                      {'spec_o': rec})
                batch_x = []
                batch_s = []
                batch_count = 0


if __name__ == "__main__":
    for inp, out in get_egs_end2end('train', model):
        print(inp['input'].shape)
#        print(inp['mask_i'].shape)
#        print(out['kmeans_o'].shape)
        print(out['spec_o'].shape)
        break
